{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Blacksujit/Health-Summerize-AI/blob/main/DoctorChatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio faiss-cpu PyPDF2 openai==0.28 tiktoken -U langchain-community"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MdoVHWQCVZZR",
        "outputId": "36fc92d5-fae6-443f-ea9d-30119bc2078c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gradio in /usr/local/lib/python3.10/dist-packages (4.44.0)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.10/dist-packages (1.8.0.post1)\n",
            "Requirement already satisfied: PyPDF2 in /usr/local/lib/python3.10/dist-packages (3.0.1)\n",
            "Requirement already satisfied: openai==0.28 in /usr/local/lib/python3.10/dist-packages (0.28.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (0.7.0)\n",
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.10/dist-packages (0.3.0)\n",
            "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (4.66.5)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai==0.28) (3.10.5)\n",
            "Requirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (23.2.1)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Requirement already satisfied: fastapi<1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.114.2)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.10/dist-packages (from gradio) (0.4.0)\n",
            "Requirement already satisfied: gradio-client==1.3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.3.0)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.27.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.24.6)\n",
            "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.4.5)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.4)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.5)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.26.4)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.10.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (24.1)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.4)\n",
            "Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (9.4.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.9.1)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.9 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.0.9)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.2)\n",
            "Requirement already satisfied: ruff>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.6.5)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: tomlkit==0.12.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.12.0)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.12.5)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.12.2)\n",
            "Requirement already satisfied: urllib3~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.0.7)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.30.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.3.0->gradio) (2024.6.1)\n",
            "Requirement already satisfied: websockets<13.0,>=10.0 in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.3.0->gradio) (12.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.5.15)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.0.34)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: langchain<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.3.0)\n",
            "Requirement already satisfied: langchain-core<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.3.0)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.112 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (0.1.120)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (2.5.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-community) (8.5.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (1.11.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28) (4.0.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (3.8)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.2.2)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.22.0)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: starlette<0.39.0,>=0.37.2 in /usr/local/lib/python3.10/dist-packages (from fastapi<1.0->gradio) (0.38.5)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (3.16.0)\n",
            "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from langchain<0.4.0,>=0.3.0->langchain-community) (0.3.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.0->langchain-community) (1.33)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.4.7)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.3 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (2.23.3)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.0.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28) (3.3.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.1.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (13.8.1)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.0->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.16.1)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HOl01myFT2Yb"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "import openai\n",
        "import faiss\n",
        "import numpy as np\n",
        "import requests\n",
        "from PyPDF2 import PdfReader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.embeddings.openai import OpenAIEmbeddings\n",
        "import json\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# OpenAI ve diğer API anahtarlarını ayarlayın\n",
        "openai_api_key = \"sk-xxxxxxxxxxxxxxxxxxxxxxxxxxxx\"  # OpenAI API anahtarınızı buraya ekleyin\n",
        "openai.api_key = openai_api_key\n",
        "\n",
        "weather_api_key = \"xxxxxxxxxxxxxxxxxxxxxxxxxxxx\"  # OpenWeatherMap API anahtarınızı buraya ekleyin\n",
        "exchange_api_key = \"xxxxxxxxxxxxxxxxxxxxxxxxxxxx\"  # Exchangeratesapi.io API anahtarınızı buraya ekleyin\n"
      ],
      "metadata": {
        "id": "IHBF4TE3VcTx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PDF dosyalarının yolları\n",
        "pdf_paths = ['/content/Current Essentials of Medicine.pdf'\n",
        "]\n",
        "\n",
        "# FAISS indeksi ve belgeler için global değişkenler\n",
        "vector_index = None\n",
        "documents = []\n",
        "chat_history = []"
      ],
      "metadata": {
        "id": "PA6W8LHvUg-f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PDF'leri okuma ve indeksleme fonksiyonu\n",
        "def index_pdfs():\n",
        "    global vector_index, documents\n",
        "\n",
        "    for pdf_path in pdf_paths:\n",
        "        pdf_reader = PdfReader(pdf_path)\n",
        "        text = \"\"\n",
        "        for page in pdf_reader.pages:\n",
        "            page_text = page.extract_text()\n",
        "            if page_text:\n",
        "                text += page_text\n",
        "        documents.append(text)\n",
        "\n",
        "    combined_text = \" \".join(documents)\n",
        "\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
        "    texts = text_splitter.split_text(combined_text)\n",
        "\n",
        "    embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
        "    vectors = embeddings.embed_documents(texts)\n",
        "\n",
        "    vector_array = np.array(vectors)\n",
        "\n",
        "    index = faiss.IndexFlatL2(vector_array.shape[1])\n",
        "    index.add(vector_array)\n",
        "\n",
        "    vector_index = index\n",
        "\n",
        "    print(\"Bilgi tabanı başarıyla oluşturuldu!\")\n",
        "\n",
        "index_pdfs()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gkqKeMEgUhBm",
        "outputId": "db6c37ca-afcd-4763-a224-44554932ecb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-b3abdacc56b4>:19: LangChainDeprecationWarning: The class `OpenAIEmbeddings` was deprecated in LangChain 0.0.9 and will be removed in 1.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import OpenAIEmbeddings`.\n",
            "  embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bilgi tabanı başarıyla oluşturuldu!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5yGSNUhFRx84"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hava durumu verilerini çekmek için fonksiyon\n",
        "def fetch_weather(location):\n",
        "    url = f\"http://api.openweathermap.org/data/2.5/weather?q={location}&appid={weather_api_key}&units=metric&lang=tr\"\n",
        "    response = requests.get(url)\n",
        "    if response.status_code == 200:\n",
        "        data = response.json()\n",
        "        temp = data['main']['temp']\n",
        "        weather_description = data['weather'][0]['description']\n",
        "        return f\"{location} için anlık hava durumu: {temp}°C ve {weather_description}.\"\n",
        "    else:\n",
        "        return \"Hava durumu bilgilerini alamadım. Lütfen konumu kontrol edip tekrar deneyin.\"\n"
      ],
      "metadata": {
        "id": "RIw1JBFuUhDt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Döviz kuru verilerini çekmek ve döviz dönüşümü yapmak için fonksiyon\n",
        "def fetch_exchange_rate(base_currency, target_currency, amount=1):\n",
        "    url = f\"https://api.exchangeratesapi.io/v1/latest?access_key={exchange_api_key}&format=1\"\n",
        "    response = requests.get(url)\n",
        "    data = response.json()\n",
        "    rates = data.get('rates', {})\n",
        "    if target_currency in rates:\n",
        "        rate = rates[target_currency]\n",
        "        converted_amount = float(amount) * rate\n",
        "        return f\"{amount} {base_currency} = {converted_amount:.2f} {target_currency}.\"\n",
        "    else:\n",
        "        return f\"{target_currency} için döviz kuru bilgisi bulunamadı.\"\n"
      ],
      "metadata": {
        "id": "4IL7v9QdUhGT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def format_chat_history(chat_history):\n",
        "    formatted_history = \"\"\n",
        "    for entry in chat_history:\n",
        "        if entry[\"role\"] == \"user\":\n",
        "            formatted_history += f\"<div class='chat-bubble user'>{entry['content']}</div>\"\n",
        "        else:\n",
        "            formatted_history += f\"<div class='chat-bubble assistant'>{entry['content']}</div>\"\n",
        "\n",
        "    return formatted_history"
      ],
      "metadata": {
        "id": "-5uDkSjnUhLB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GPT-4 Yanıtını oluşturmak için fonksiyon (function calling ile)\n",
        "def generate_gpt4_response(prompt_input):\n",
        "    global vector_index, documents, chat_history\n",
        "    openai.api_key = openai_api_key\n",
        "\n",
        "    functions = [\n",
        "        {\n",
        "            \"name\": \"fetch_weather\",\n",
        "            \"description\": \"Belirli bir konum için hava durumu bilgisini alır.\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"location\": {\n",
        "                        \"type\": \"string\",\n",
        "                        \"description\": \"Hava durumu almak istediğiniz konumun adı\"\n",
        "                    }\n",
        "                },\n",
        "                \"required\": [\"location\"],\n",
        "            },\n",
        "        },\n",
        "        {\n",
        "            \"name\": \"fetch_exchange_rate\",\n",
        "            \"description\": \"Belirli iki para birimi arasındaki döviz kuru bilgisini alır ve isteğe bağlı olarak belirli bir miktar için dönüştürme yapar.\",\n",
        "            \"parameters\": {\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                    \"base_currency\": {\n",
        "                        \"type\": \"string\",\n",
        "                        \"description\": \"Döviz kurunu almak istediğiniz temel para birimi, varsayılan olarak EUR'dir.\"\n",
        "                    },\n",
        "                    \"target_currency\": {\n",
        "                        \"type\": \"string\",\n",
        "                        \"description\": \"Döviz kurunu almak istediğiniz hedef para biriminin ISO 4217 kodu (örneğin: TRY)\"\n",
        "                    },\n",
        "                    \"amount\": {\n",
        "                        \"type\": \"number\",\n",
        "                        \"description\": \"Dönüştürmek istediğiniz miktar (varsayılan olarak 1).\"\n",
        "                    }\n",
        "                },\n",
        "                \"required\": [\"base_currency\", \"target_currency\", \"amount\"],\n",
        "            },\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    # OpenAI API çağrısı (function calling ile)\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model='gpt-4o-mini',\n",
        "        messages=[\n",
        "            {\"role\": \"system\",\n",
        "             \"content\": \"Sen tıp bilgileri ile donatılmış bir asistansın ve görevin tıbbi konulardaki sorulara cevap vermektir.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt_input}\n",
        "        ],\n",
        "        functions=functions,\n",
        "        function_call=\"auto\",  # Modelin fonksiyon çağrısına karar vermesine izin ver\n",
        "        temperature=0.5,\n",
        "        max_tokens=512\n",
        "    )\n",
        "\n",
        "    # Sohbet geçmişini güncelle\n",
        "    chat_history.append({\"role\": \"user\", \"content\": prompt_input})\n",
        "\n",
        "    # Bir fonksiyon çağrısı istenip istenmediğini kontrol et\n",
        "    if 'choices' in response and response['choices'][0]['finish_reason'] == 'function_call':\n",
        "        function_call_info = response['choices'][0]['message']['function_call']\n",
        "        function_name = function_call_info['name']\n",
        "        arguments = json.loads(function_call_info['arguments'])\n",
        "\n",
        "        if function_name == 'fetch_weather':\n",
        "            location = arguments['location']\n",
        "            weather_response = fetch_weather(location)\n",
        "            chat_history.append({\"role\": \"assistant\", \"content\": weather_response})\n",
        "            return format_chat_history(chat_history)\n",
        "        elif function_name == 'fetch_exchange_rate':\n",
        "            base_currency = arguments['base_currency']\n",
        "            target_currency = arguments['target_currency']\n",
        "            amount = arguments.get('amount', 1)\n",
        "            exchange_rate_response = fetch_exchange_rate(base_currency, target_currency, amount)\n",
        "            chat_history.append({\"role\": \"assistant\", \"content\": exchange_rate_response})\n",
        "            return format_chat_history(chat_history)\n",
        "\n",
        "    # Asistanın yanıtını al\n",
        "    assistant_response = response['choices'][0]['message']['content'].strip()\n",
        "    chat_history.append({\"role\": \"assistant\", \"content\": assistant_response})\n",
        "\n",
        "    return format_chat_history(chat_history)"
      ],
      "metadata": {
        "id": "k72356afUhIn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with gr.Blocks() as demo:\n",
        "    # CSS ile kaydırma özelliği ve stil düzenlemeleri\n",
        "    custom_css = \"\"\"\n",
        "    /* Chat balonları için stil */\n",
        "    .chat-bubble {\n",
        "        padding: 10px;\n",
        "        border-radius: 10px;\n",
        "        margin-bottom: 10px;\n",
        "        max-width: 60%;\n",
        "        word-wrap: break-word;\n",
        "    }\n",
        "\n",
        "    .user {\n",
        "        background-color: #d1e7dd;\n",
        "        text-align: right;\n",
        "        margin-left: auto;\n",
        "    }\n",
        "\n",
        "    .assistant {\n",
        "        background-color: #f8d7da;\n",
        "        text-align: left;\n",
        "        margin-right: auto;\n",
        "    }\n",
        "\n",
        "    /* Kaydırılabilir sohbet kutusu */\n",
        "    #output-box {\n",
        "        height: 400px;  /* Sabit yükseklik */\n",
        "        width: 100%;\n",
        "        overflow-y: scroll !important;  /* Kaydırmayı gizle ama JS ile açacağız */\n",
        "        padding: 10px;\n",
        "        border: 1px solid #ccc;\n",
        "        border-radius: 10px;\n",
        "        background-color: #f8f9fa;\n",
        "        margin-bottom: 20px;\n",
        "    }\n",
        "\n",
        "    /* Giriş kutusu */\n",
        "    #input-box {\n",
        "        height: 150px;\n",
        "        width: 100%;\n",
        "    }\n",
        "    \"\"\"\n",
        "\n",
        "    output_textbox = gr.HTML(label=\"Yanıt\", elem_id=\"output-box\")  # HTML bileşeni ile sohbeti gösteriyoruz\n",
        "    input_textbox = gr.Textbox(label=\"Sorunuzu girin\", lines=4, elem_id=\"input-box\")  # Giriş alanı\n",
        "\n",
        "    # Sorgu gönderildiğinde çalıştırılacak fonksiyon\n",
        "    def on_submit(prompt_input):\n",
        "        response = generate_gpt4_response(prompt_input)\n",
        "        return response, \"\"  # Giriş kutusunu temizle\n",
        "\n",
        "    # Sohbeti temizlemek için fonksiyon\n",
        "    def clear_chat():\n",
        "        global chat_history\n",
        "        chat_history.clear()\n",
        "        return \"\", \"\"  # Hem giriş kutusunu hem de sohbeti temizle\n",
        "\n",
        "    # Giriş kutusuna \"Enter\" basıldığında çalıştırılan fonksiyon\n",
        "    input_textbox.submit(on_submit, inputs=input_textbox, outputs=[output_textbox, input_textbox])\n",
        "\n",
        "    # Mesaj gönderme ve sohbeti temizleme butonları\n",
        "    submit_btn = gr.Button(\"Gönder\")\n",
        "    clear_btn = gr.Button(\"Chat'i Temizle\")\n",
        "\n",
        "    submit_btn.click(on_submit, inputs=input_textbox, outputs=[output_textbox, input_textbox])\n",
        "    clear_btn.click(clear_chat, outputs=[output_textbox, input_textbox])\n",
        "\n",
        "    demo.css = custom_css\n",
        "    demo.launch(share=True, debug=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 745
        },
        "id": "xNYH5O4hUhV1",
        "outputId": "80218d42-6523-456b-fc7b-d3ee8f6c2a02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Running on public URL: https://1046f76164fb165e08.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://1046f76164fb165e08.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://b162ad70725dccd76c.gradio.live\n",
            "Killing tunnel 127.0.0.1:7861 <> https://2eae138fb007aae19d.gradio.live\n",
            "Killing tunnel 127.0.0.1:7862 <> https://1dcffbd5ed4db3ce8d.gradio.live\n",
            "Killing tunnel 127.0.0.1:7863 <> https://c9b054571b98902445.gradio.live\n",
            "Killing tunnel 127.0.0.1:7864 <> https://6311d0f05878886aba.gradio.live\n",
            "Killing tunnel 127.0.0.1:7865 <> https://44bc8391d8b84b0e1d.gradio.live\n",
            "Killing tunnel 127.0.0.1:7866 <> https://1046f76164fb165e08.gradio.live\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio transformers torch pillow"
      ],
      "metadata": {
        "id": "HbHxZ2VzVKkb",
        "outputId": "74f2bfbe-4173-4a7b-a0a9-2dd03b96cf77",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gradio\n",
            "  Downloading gradio-5.29.0-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (11.2.1)\n",
            "Collecting aiofiles<25.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-24.1.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\n",
            "Collecting fastapi<1.0,>=0.115.2 (from gradio)\n",
            "  Downloading fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting gradio-client==1.10.0 (from gradio)\n",
            "  Downloading gradio_client-1.10.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting groovy~=0.1 (from gradio)\n",
            "  Downloading groovy-0.1.2-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.31.1)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.18)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.11.4)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart>=0.0.18 (from gradio)\n",
            "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Collecting ruff>=0.9.3 (from gradio)\n",
            "  Downloading ruff-0.11.9-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio)\n",
            "  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
            "  Downloading starlette-0.46.2-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.15.3)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.13.2)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.34.2-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.0->gradio) (2025.3.2)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.0->gradio) (15.0.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (1.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Downloading gradio-5.29.0-py3-none-any.whl (54.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.1/54.1 MB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.10.0-py3-none-any.whl (322 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.9/322.9 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m50.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m47.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m84.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-24.1.0-py3-none-any.whl (15 kB)\n",
            "Downloading fastapi-0.115.12-py3-none-any.whl (95 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading groovy-0.1.2-py3-none-any.whl (14 kB)\n",
            "Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
            "Downloading ruff-0.11.9-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.5/11.5 MB\u001b[0m \u001b[31m105.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
            "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading starlette-0.46.2-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
            "Downloading uvicorn-0.34.2-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub, uvicorn, tomlkit, semantic-version, ruff, python-multipart, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, groovy, ffmpy, aiofiles, starlette, nvidia-cusparse-cu12, nvidia-cudnn-cu12, safehttpx, nvidia-cusolver-cu12, gradio-client, fastapi, gradio\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed aiofiles-24.1.0 fastapi-0.115.12 ffmpy-0.5.0 gradio-5.29.0 gradio-client-1.10.0 groovy-0.1.2 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 pydub-0.25.1 python-multipart-0.0.20 ruff-0.11.9 safehttpx-0.1.6 semantic-version-2.10.0 starlette-0.46.2 tomlkit-0.13.2 uvicorn-0.34.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "from transformers import BlipProcessor, BlipForConditionalGeneration, AutoModelForCausalLM, AutoTokenizer\n",
        "from PIL import Image, ImageOps\n",
        "import torch\n",
        "import logging\n",
        "from datetime import datetime\n",
        "import re\n",
        "import os\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
        "    filename='medical_analysis.log'\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Ensure offline mode for transformers\n",
        "os.environ[\"TRANSFORMERS_OFFLINE\"] = \"1\"\n",
        "\n",
        "class MedicalImageAnalyzer:\n",
        "    def __init__(self):\n",
        "        \"\"\"Initialize the medical image analysis system.\"\"\"\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "        self.image_processor = None\n",
        "        self.image_model = None\n",
        "        self.nlp_model = None\n",
        "        self.nlp_tokenizer = None\n",
        "        self.load_models()\n",
        "\n",
        "    def load_models(self):\n",
        "        \"\"\"Load all required models locally.\"\"\"\n",
        "        try:\n",
        "            logger.info(\"Loading BLIP model for image analysis...\")\n",
        "            self.image_processor = BlipProcessor.from_pretrained(\n",
        "                \"Salesforce/blip-image-captioning-base\",\n",
        "                cache_dir=\"./model_cache\"\n",
        "            )\n",
        "            self.image_model = BlipForConditionalGeneration.from_pretrained(\n",
        "                \"Salesforce/blip-image-captioning-base\",\n",
        "                cache_dir=\"./model_cache\"\n",
        "            ).to(self.device)\n",
        "\n",
        "            logger.info(\"Loading local NLP model...\")\n",
        "            self.nlp_tokenizer = AutoTokenizer.from_pretrained(\n",
        "                \"stanford-crfm/BioMedLM\",\n",
        "                cache_dir=\"./model_cache\"\n",
        "            )\n",
        "            self.nlp_model = AutoModelForCausalLM.from_pretrained(\n",
        "                \"stanford-crfm/BioMedLM\",\n",
        "                cache_dir=\"./model_cache\"\n",
        "            ).to(self.device)\n",
        "\n",
        "            logger.info(\"All models loaded successfully.\")\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Model loading failed: {str(e)}\")\n",
        "            raise RuntimeError(\"Failed to initialize models. Ensure models are available locally.\")\n",
        "\n",
        "    def analyze_image(self, image: Image.Image, clinical_context: str = \"\") -> dict:\n",
        "        \"\"\"Analyze medical image with clinical context.\"\"\"\n",
        "        try:\n",
        "            if not image:\n",
        "                return {\"error\": \"No image provided\"}\n",
        "\n",
        "            # Preprocess image\n",
        "            processed_image = self._preprocess_image(image)\n",
        "\n",
        "            # Generate findings\n",
        "            findings = self._generate_findings(processed_image, clinical_context)\n",
        "\n",
        "            # Generate recommendations\n",
        "            recommendations = self._generate_recommendations(findings)\n",
        "\n",
        "            # Structure the final report\n",
        "            report = self._structure_report(clinical_context, findings, recommendations)\n",
        "\n",
        "            return report\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Analysis failed: {str(e)}\")\n",
        "            return {\"error\": str(e)}\n",
        "\n",
        "    def _preprocess_image(self, image: Image.Image) -> Image.Image:\n",
        "        \"\"\"Preprocess medical image for analysis.\"\"\"\n",
        "        try:\n",
        "            if image.mode != 'RGB':\n",
        "                image = image.convert('RGB')\n",
        "            image = ImageOps.exif_transpose(image)\n",
        "            return image\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Image preprocessing failed: {str(e)}\")\n",
        "            raise RuntimeError(\"Image preprocessing failed.\")\n",
        "\n",
        "    def _generate_findings(self, image: Image.Image, context: str) -> str:\n",
        "        \"\"\"Generate findings from the image.\"\"\"\n",
        "        try:\n",
        "            prompt = (\n",
        "                f\"Analyze the medical image and provide a detailed report. \"\n",
        "                f\"Clinical context: {context if context else 'None provided'}. \"\n",
        "                \"Include details about anatomical structures, abnormalities, and technical quality.\"\n",
        "            )\n",
        "            inputs = self.image_processor(image, text=prompt, return_tensors=\"pt\").to(self.device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = self.image_model.generate(**inputs, max_new_tokens=200)\n",
        "\n",
        "            findings = self.image_processor.decode(outputs[0], skip_special_tokens=True)\n",
        "            return self._clean_text(findings)\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Findings generation failed: {str(e)}\")\n",
        "            raise RuntimeError(\"Could not generate findings.\")\n",
        "\n",
        "    def _generate_recommendations(self, findings: str) -> str:\n",
        "        \"\"\"Generate clinical recommendations.\"\"\"\n",
        "        try:\n",
        "            prompt = (\n",
        "                f\"Based on the following findings:\\n{findings}\\n\\n\"\n",
        "                \"Provide 3-5 clinical recommendations. Categorize them as urgent or routine.\"\n",
        "            )\n",
        "            inputs = self.nlp_tokenizer(prompt, return_tensors=\"pt\").to(self.device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = self.nlp_model.generate(**inputs, max_new_tokens=150, temperature=0.7)\n",
        "\n",
        "            recommendations = self.nlp_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "            return self._clean_text(recommendations)\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Recommendation generation failed: {str(e)}\")\n",
        "            return \"Could not generate recommendations.\"\n",
        "\n",
        "    def _structure_report(self, context: str, findings: str, recommendations: str) -> dict:\n",
        "        \"\"\"Structure the final report.\"\"\"\n",
        "        return {\n",
        "            \"metadata\": {\n",
        "                \"report_date\": datetime.now().isoformat(),\n",
        "                \"analysis_version\": \"1.0\"\n",
        "            },\n",
        "            \"clinical_context\": context,\n",
        "            \"findings\": findings,\n",
        "            \"recommendations\": recommendations\n",
        "        }\n",
        "\n",
        "    def _clean_text(self, text: str) -> str:\n",
        "        \"\"\"Clean up the generated text.\"\"\"\n",
        "        text = text.strip()\n",
        "        text = re.sub(r\"\\s+\", \" \", text)  # Remove extra whitespace\n",
        "        return text\n",
        "\n",
        "# Initialize the analyzer\n",
        "try:\n",
        "    analyzer = MedicalImageAnalyzer()\n",
        "except Exception as e:\n",
        "    logger.critical(f\"System initialization failed: {str(e)}\")\n",
        "    raise RuntimeError(\"Medical analysis system failed to initialize.\")\n",
        "\n",
        "def analyze_medical_image(image, clinical_context=\"\"):\n",
        "    \"\"\"Wrapper function for Gradio interface.\"\"\"\n",
        "    try:\n",
        "        if not image:\n",
        "            return \"⚠️ Please upload a medical image.\"\n",
        "\n",
        "        # Perform analysis\n",
        "        report = analyzer.analyze_image(image, clinical_context)\n",
        "\n",
        "        if \"error\" in report:\n",
        "            return f\"❌ Error: {report['error']}\"\n",
        "\n",
        "        # Format the report\n",
        "        return (\n",
        "            f\"**Clinical Context**: {clinical_context if clinical_context else 'None provided'}\\n\\n\"\n",
        "            f\"**Findings**:\\n{report['findings']}\\n\\n\"\n",
        "            f\"**Recommendations**:\\n{report['recommendations']}\\n\\n\"\n",
        "            \"Note: This analysis requires verification by a qualified radiologist.\"\n",
        "        )\n",
        "    except Exception as e:\n",
        "        return f\"❌ System Error: {str(e)}\"\n",
        "\n",
        "# Gradio Interface\n",
        "with gr.Blocks(theme=gr.themes.Soft(), title=\"Medical Image Analysis\") as app:\n",
        "    gr.Markdown(\"# 🩺 Medical Image Analysis\")\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            image_input = gr.Image(type=\"pil\", label=\"Upload Medical Image\")\n",
        "            context_input = gr.Textbox(label=\"Clinical Context (optional)\", placeholder=\"Patient symptoms or history...\")\n",
        "            analyze_btn = gr.Button(\"Analyze\", variant=\"primary\")\n",
        "        with gr.Column():\n",
        "            report_output = gr.Markdown(label=\"Analysis Report\")\n",
        "\n",
        "    analyze_btn.click(\n",
        "        analyze_medical_image,\n",
        "        inputs=[image_input, context_input],\n",
        "        outputs=[report_output]\n",
        "    )\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    app.launch(server_name=\"0.0.0.0\", server_port=78)"
      ],
      "metadata": {
        "id": "XdxM_G07VKmy",
        "outputId": "b1163942-eee1-480d-e454-33c4b240dfda",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 650
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://ed690cf29681d2f068.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://ed690cf29681d2f068.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required libraries\n",
        "!pip install gradio transformers torch pillow datasets\n",
        "\n",
        "# Import necessary libraries\n",
        "import os\n",
        "import logging\n",
        "import re\n",
        "from datetime import datetime\n",
        "from PIL import Image, ImageOps\n",
        "import torch\n",
        "from transformers import (\n",
        "    BlipProcessor,\n",
        "    BlipForConditionalGeneration,\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        ")\n",
        "from datasets import load_dataset\n",
        "import gradio as gr\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Set device\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Step 1: Fine-Tune the BLIP Model for Image Analysis\n",
        "def fine_tune_blip():\n",
        "    logger.info(\"Loading spinal cord dataset...\")\n",
        "    dataset_path = \"/content/spinal_cord_dataset\"  # Kaggle dataset path\n",
        "    dataset = load_dataset(\"imagefolder\", data_dir=dataset_path, split=\"train[:1%]\")  # Use a small subset for demonstration\n",
        "\n",
        "    logger.info(\"Loading BLIP model and processor...\")\n",
        "    processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "    model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\").to(device)\n",
        "\n",
        "    def preprocess_function(examples):\n",
        "        inputs = processor(text=examples[\"label\"], images=examples[\"image\"], return_tensors=\"pt\", padding=True)\n",
        "        return inputs\n",
        "\n",
        "    logger.info(\"Preprocessing dataset...\")\n",
        "    tokenized_dataset = dataset.map(preprocess_function, batched=True)\n",
        "\n",
        "    logger.info(\"Setting up training arguments...\")\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=\"./blip_results\",\n",
        "        evaluation_strategy=\"epoch\",\n",
        "        learning_rate=5e-5,\n",
        "        per_device_train_batch_size=4,\n",
        "        num_train_epochs=1,  # Set to 1 for demonstration; increase for real training\n",
        "        save_steps=10_000,\n",
        "        save_total_limit=2,\n",
        "        remove_unused_columns=False,\n",
        "    )\n",
        "\n",
        "    logger.info(\"Starting fine-tuning...\")\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=tokenized_dataset,\n",
        "    )\n",
        "    trainer.train()\n",
        "\n",
        "    logger.info(\"Fine-tuning complete. Saving model...\")\n",
        "    model.save_pretrained(\"./fine_tuned_blip\")\n",
        "    processor.save_pretrained(\"./fine_tuned_blip\")\n",
        "\n",
        "# Step 2: Fine-Tune the GPT-2 Model for Text Generation\n",
        "def fine_tune_gpt2():\n",
        "    logger.info(\"Loading radiology reports dataset...\")\n",
        "    dataset_path = \"/content/spinal_cord_dataset\"  # Kaggle dataset path\n",
        "    dataset = load_dataset(\"text\", data_files={\"train\": os.path.join(dataset_path, \"reports.txt\")}, split=\"train\")\n",
        "\n",
        "    logger.info(\"Loading GPT-2 model and tokenizer...\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "    model = AutoModelForCausalLM.from_pretrained(\"gpt2\").to(device)\n",
        "\n",
        "    def preprocess_function(examples):\n",
        "        return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=512)\n",
        "\n",
        "    logger.info(\"Preprocessing dataset...\")\n",
        "    tokenized_dataset = dataset.map(preprocess_function, batched=True)\n",
        "\n",
        "    logger.info(\"Setting up training arguments...\")\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=\"./gpt2_results\",\n",
        "        evaluation_strategy=\"epoch\",\n",
        "        learning_rate=5e-5,\n",
        "        per_device_train_batch_size=4,\n",
        "        num_train_epochs=1,  # Set to 1 for demonstration; increase for real training\n",
        "        save_steps=10_000,\n",
        "        save_total_limit=2,\n",
        "    )\n",
        "\n",
        "    logger.info(\"Starting fine-tuning...\")\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=tokenized_dataset,\n",
        "    )\n",
        "    trainer.train()\n",
        "\n",
        "    logger.info(\"Fine-tuning complete. Saving model...\")\n",
        "    model.save_pretrained(\"./fine_tuned_gpt2\")\n",
        "    tokenizer.save_pretrained(\"./fine_tuned_gpt2\")\n",
        "\n",
        "# Step 3: Load Fine-Tuned Models for Inference\n",
        "class MedicalImageAnalyzer:\n",
        "    def __init__(self):\n",
        "        logger.info(\"Loading fine-tuned BLIP model...\")\n",
        "        self.image_processor = BlipProcessor.from_pretrained(\"./fine_tuned_blip\")\n",
        "        self.image_model = BlipForConditionalGeneration.from_pretrained(\"./fine_tuned_blip\").to(device)\n",
        "\n",
        "        logger.info(\"Loading fine-tuned GPT-2 model...\")\n",
        "        self.nlp_tokenizer = AutoTokenizer.from_pretrained(\"./fine_tuned_gpt2\")\n",
        "        self.nlp_model = AutoModelForCausalLM.from_pretrained(\"./fine_tuned_gpt2\").to(device)\n",
        "\n",
        "    def analyze_image(self, image: Image.Image, clinical_context: str = \"\") -> dict:\n",
        "        try:\n",
        "            if not image:\n",
        "                return {\"error\": \"No image provided\"}\n",
        "\n",
        "            # Preprocess image\n",
        "            processed_image = self._preprocess_image(image)\n",
        "\n",
        "            # Generate findings\n",
        "            findings = self._generate_findings(processed_image, clinical_context)\n",
        "\n",
        "            # Generate recommendations\n",
        "            recommendations = self._generate_recommendations(findings)\n",
        "\n",
        "            # Structure the final report\n",
        "            report = self._structure_report(clinical_context, findings, recommendations)\n",
        "\n",
        "            return report\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Analysis failed: {str(e)}\")\n",
        "            return {\"error\": str(e)}\n",
        "\n",
        "    def _preprocess_image(self, image: Image.Image) -> Image.Image:\n",
        "        if image.mode != \"RGB\":\n",
        "            image = image.convert(\"RGB\")\n",
        "        return ImageOps.exif_transpose(image)\n",
        "\n",
        "    def _generate_findings(self, image: Image.Image, context: str) -> str:\n",
        "        prompt = (\n",
        "            f\"Analyze the medical image and provide a detailed report. \"\n",
        "            f\"Clinical context: {context if context else 'None provided'}. \"\n",
        "            \"Include details about anatomical structures, abnormalities, and technical quality.\"\n",
        "        )\n",
        "        inputs = self.image_processor(image, text=prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = self.image_model.generate(**inputs, max_new_tokens=200)\n",
        "\n",
        "        findings = self.image_processor.decode(outputs[0], skip_special_tokens=True)\n",
        "        return findings.strip()\n",
        "\n",
        "    def _generate_recommendations(self, findings: str) -> str:\n",
        "        prompt = (\n",
        "            f\"Based on the following findings:\\n{findings}\\n\\n\"\n",
        "            \"Provide 3-5 clinical recommendations. Categorize them as urgent or routine.\"\n",
        "        )\n",
        "        inputs = self.nlp_tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = self.nlp_model.generate(**inputs, max_new_tokens=150, temperature=0.7)\n",
        "\n",
        "        recommendations = self.nlp_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        return recommendations.strip()\n",
        "\n",
        "    def _structure_report(self, context: str, findings: str, recommendations: str) -> dict:\n",
        "        return {\n",
        "            \"metadata\": {\n",
        "                \"report_date\": datetime.now().isoformat(),\n",
        "                \"analysis_version\": \"1.0\",\n",
        "            },\n",
        "            \"clinical_context\": context,\n",
        "            \"findings\": findings,\n",
        "            \"recommendations\": recommendations,\n",
        "        }\n",
        "\n",
        "# Step 4: Deploy the Model on Gradio\n",
        "def analyze_medical_image(image, clinical_context=\"\"):\n",
        "    analyzer = MedicalImageAnalyzer()\n",
        "    report = analyzer.analyze_image(image, clinical_context)\n",
        "\n",
        "    if \"error\" in report:\n",
        "        return f\"❌ Error: {report['error']}\"\n",
        "\n",
        "    return (\n",
        "        f\"**Clinical Context**: {clinical_context if clinical_context else 'None provided'}\\n\\n\"\n",
        "        f\"**Findings**:\\n{report['findings']}\\n\\n\"\n",
        "        f\"**Recommendations**:\\n{report['recommendations']}\\n\\n\"\n",
        "        \"Note: This analysis requires verification by a qualified radiologist.\"\n",
        "    )\n",
        "\n",
        "with gr.Blocks() as app:\n",
        "    gr.Markdown(\"# 🩺 Medical Image Analysis\")\n",
        "    with gr.Row():\n",
        "        with gr.Column():\n",
        "            image_input = gr.Image(type=\"pil\", label=\"Upload Medical Image\")\n",
        "            context_input = gr.Textbox(label=\"Clinical Context (optional)\", placeholder=\"Patient symptoms or history...\")\n",
        "            analyze_btn = gr.Button(\"Analyze\", variant=\"primary\")\n",
        "        with gr.Column():\n",
        "            report_output = gr.Markdown(label=\"Analysis Report\")\n",
        "\n",
        "    analyze_btn.click(\n",
        "        analyze_medical_image,\n",
        "        inputs=[image_input, context_input],\n",
        "        outputs=[report_output],\n",
        "    )\n",
        "\n",
        "app.launch(server_name=\"0.0.0.0\", server_port=6)"
      ],
      "metadata": {
        "id": "gTICnappVKpn",
        "outputId": "0df01800-e0a4-4af3-d4bc-ece66f6ee532",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gradio\n",
            "  Downloading gradio-5.29.0-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (11.2.1)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.14.4)\n",
            "Collecting aiofiles<25.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-24.1.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\n",
            "Collecting fastapi<1.0,>=0.115.2 (from gradio)\n",
            "  Downloading fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting gradio-client==1.10.0 (from gradio)\n",
            "  Downloading gradio_client-1.10.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting groovy~=0.1 (from gradio)\n",
            "  Downloading groovy-0.1.2-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.31.1)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.18)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.11.4)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart>=0.0.18 (from gradio)\n",
            "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Collecting ruff>=0.9.3 (from gradio)\n",
            "  Downloading ruff-0.11.9-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio)\n",
            "  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
            "  Downloading starlette-0.46.2-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.15.3)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.13.2)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.34.2-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.0->gradio) (2025.3.2)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.0->gradio) (15.0.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (1.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Downloading gradio-5.29.0-py3-none-any.whl (54.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.1/54.1 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.10.0-py3-none-any.whl (322 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.9/322.9 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m59.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m85.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-24.1.0-py3-none-any.whl (15 kB)\n",
            "Downloading fastapi-0.115.12-py3-none-any.whl (95 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading groovy-0.1.2-py3-none-any.whl (14 kB)\n",
            "Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
            "Downloading ruff-0.11.9-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.5/11.5 MB\u001b[0m \u001b[31m78.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
            "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading starlette-0.46.2-py3-none-any.whl (72 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
            "Downloading uvicorn-0.34.2-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub, uvicorn, tomlkit, semantic-version, ruff, python-multipart, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, groovy, ffmpy, aiofiles, starlette, nvidia-cusparse-cu12, nvidia-cudnn-cu12, safehttpx, nvidia-cusolver-cu12, gradio-client, fastapi, gradio\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed aiofiles-24.1.0 fastapi-0.115.12 ffmpy-0.5.0 gradio-5.29.0 gradio-client-1.10.0 groovy-0.1.2 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 pydub-0.25.1 python-multipart-0.0.20 ruff-0.11.9 safehttpx-0.1.6 semantic-version-2.10.0 starlette-0.46.2 tomlkit-0.13.2 uvicorn-0.34.2\n",
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://93da52510aecfb1733.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://93da52510aecfb1733.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# spinal_cord_dataset_path = !kagglehub.dataset_download('trainingdatapro/spinal-cord-dataset')"
      ],
      "metadata": {
        "id": "mtc_Bf0iewQW"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kagglehub\n",
        "# Install Kaggle API\n",
        "!pip install kaggle"
      ],
      "metadata": {
        "id": "XePW1fsTeyQ4",
        "outputId": "c97a3a92-45c8-46a6-81d9-7bafaa85a542",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kagglehub in /usr/local/lib/python3.11/dist-packages (0.3.12)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from kagglehub) (24.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from kagglehub) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kagglehub) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kagglehub) (4.67.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (2025.4.26)\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.11/dist-packages (1.7.4.5)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.11/dist-packages (from kaggle) (6.2.0)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2025.4.26)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.4.2)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from kaggle) (5.29.4)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.9.0.post0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.11/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.32.3)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.11/dist-packages (from kaggle) (75.2.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.17.0)\n",
            "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kaggle) (4.67.1)\n",
            "Requirement already satisfied: urllib3>=1.15.1 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.4.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from kaggle) (0.5.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Upload kaggle.json\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json\n",
        "\n"
      ],
      "metadata": {
        "id": "6r4whBEQe1DS",
        "outputId": "c2294aff-29ab-426f-f1ea-8113d3105d8d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cp: cannot stat 'kaggle.json': No such file or directory\n",
            "chmod: cannot access '/root/.kaggle/kaggle.json': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the dataset\n",
        "!kaggle datasets download -d trainingdatapro/spinal-cord-dataset\n"
      ],
      "metadata": {
        "id": "irvs9LRxfr59",
        "outputId": "20e06f63-83d9-441e-82e1-8a459bf9e2c1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/kaggle\", line 10, in <module>\n",
            "    sys.exit(main())\n",
            "             ^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/kaggle/cli.py\", line 68, in main\n",
            "    out = args.func(**command_args)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/kaggle/api/kaggle_api_extended.py\", line 1741, in dataset_download_cli\n",
            "    with self.build_kaggle_client() as kaggle:\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/kaggle/api/kaggle_api_extended.py\", line 688, in build_kaggle_client\n",
            "    username=self.config_values['username'],\n",
            "             ~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^\n",
            "KeyError: 'username'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "# from kaggle.api.kaggle_api_extended import KaggleApi\n",
        "\n",
        "# # Set up Kaggle API credentials\n",
        "# os.environ[\"KAGGLE_USERNAME\"] = \"sujitnirmal\"\n",
        "# os.environ[\"KAGGLE_KEY\"] = \"591a53c307288d0b46fc0a7e25d60531\"\n",
        "\n",
        "# # Initialize the Kaggle API\n",
        "# api = KaggleApi()\n",
        "# api.authenticate()\n",
        "\n",
        "# # Download the spinal-cord-dataset\n",
        "# dataset_name = \"trainingdatapro/spinal-cord-dataset\"\n",
        "# download_path = \"./spinal_cord_dataset\"\n",
        "# api.dataset_download_files(dataset_name, path=download_path, unzip=True)\n",
        "\n",
        "# print(f\"Dataset downloaded and extracted to: {download_path}\")"
      ],
      "metadata": {
        "id": "K9AFJOxjfuDt"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required libraries\n",
        "!pip install gradio transformers torch pillow datasets\n",
        "\n",
        "# Import necessary libraries\n",
        "import os\n",
        "import logging\n",
        "import re\n",
        "from datetime import datetime\n",
        "from PIL import Image, ImageOps\n",
        "import torch\n",
        "from transformers import (\n",
        "    BlipProcessor,\n",
        "    BlipForConditionalGeneration,\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        ")\n",
        "from datasets import load_dataset\n",
        "import gradio as gr\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Set device\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "logger.info(f\"Using device: {device}\")\n",
        "\n",
        "# Define paths\n",
        "BLIP_MODEL_PATH = \"./fine_tuned_blip\"\n",
        "GPT2_MODEL_PATH = \"./fine_tuned_gpt2\"\n",
        "DATASET_PATH = \"/content/spinal_cord_dataset/ST000001\"  # Update this to your actual dataset path\n",
        "\n",
        "# Step 1: Fine-Tune the BLIP Model for Image Analysis\n",
        "def fine_tune_blip(dataset_path=DATASET_PATH, force_retrain=False):\n",
        "    \"\"\"Fine-tune BLIP model if not already fine-tuned or if force_retrain is True.\"\"\"\n",
        "    if os.path.exists(BLIP_MODEL_PATH) and not force_retrain:\n",
        "        logger.info(f\"BLIP model already fine-tuned at {BLIP_MODEL_PATH}. Skipping training.\")\n",
        "        return\n",
        "\n",
        "    # Ensure dataset directory exists\n",
        "    if not os.path.exists(dataset_path):\n",
        "        logger.error(f\"Dataset not found at {dataset_path}. Please download the dataset first.\")\n",
        "        raise FileNotFoundError(f\"Dataset not found at {dataset_path}\")\n",
        "\n",
        "    try:\n",
        "        logger.info(\"Loading spinal cord dataset...\")\n",
        "        # Try to load as image folder dataset\n",
        "        try:\n",
        "            dataset = load_dataset(\"imagefolder\", data_dir=dataset_path, split=\"train[:1%]\")\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Failed to load dataset as imagefolder: {e}\")\n",
        "            # Fallback to using a different method or path structure\n",
        "            dataset = load_dataset(dataset_path, split=\"train[:1%]\")\n",
        "\n",
        "        logger.info(\"Loading BLIP model and processor...\")\n",
        "        processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "        model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\").to(device)\n",
        "\n",
        "        def preprocess_function(examples):\n",
        "            # Ensure 'label' exists in the dataset, otherwise use empty strings\n",
        "            labels = examples.get(\"label\", [\"\"] * len(examples[\"image\"]))\n",
        "            inputs = processor(images=examples[\"image\"], text=labels, return_tensors=\"pt\", padding=True)\n",
        "            return inputs\n",
        "\n",
        "        logger.info(\"Preprocessing dataset...\")\n",
        "        tokenized_dataset = dataset.map(preprocess_function, batched=True, remove_columns=dataset.column_names)\n",
        "\n",
        "        logger.info(\"Setting up training arguments...\")\n",
        "        training_args = TrainingArguments(\n",
        "            output_dir=\"./blip_results\",\n",
        "            evaluation_strategy=\"epoch\",\n",
        "            learning_rate=5e-5,\n",
        "            per_device_train_batch_size=4,\n",
        "            num_train_epochs=1,  # Set to 1 for demonstration; increase for real training\n",
        "            save_steps=10_000,\n",
        "            save_total_limit=2,\n",
        "            remove_unused_columns=False,\n",
        "        )\n",
        "\n",
        "        logger.info(\"Starting fine-tuning...\")\n",
        "        trainer = Trainer(\n",
        "            model=model,\n",
        "            args=training_args,\n",
        "            train_dataset=tokenized_dataset,\n",
        "        )\n",
        "        trainer.train()\n",
        "\n",
        "        logger.info(\"Fine-tuning complete. Saving model...\")\n",
        "        model.save_pretrained(BLIP_MODEL_PATH)\n",
        "        processor.save_pretrained(BLIP_MODEL_PATH)\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Failed to fine-tune BLIP model: {e}\")\n",
        "        raise\n",
        "\n",
        "# Step 2: Fine-Tune the GPT-2 Model for Text Generation\n",
        "def fine_tune_gpt2(dataset_path=DATASET_PATH, force_retrain=False):\n",
        "    \"\"\"Fine-tune GPT-2 model if not already fine-tuned or if force_retrain is True.\"\"\"\n",
        "    if os.path.exists(GPT2_MODEL_PATH) and not force_retrain:\n",
        "        logger.info(f\"GPT-2 model already fine-tuned at {GPT2_MODEL_PATH}. Skipping training.\")\n",
        "        return\n",
        "\n",
        "    # Ensure dataset directory exists\n",
        "    if not os.path.exists(dataset_path):\n",
        "        logger.error(f\"Dataset not found at {dataset_path}. Please download the dataset first.\")\n",
        "        raise FileNotFoundError(f\"Dataset not found at {dataset_path}\")\n",
        "\n",
        "    reports_file = os.path.join(dataset_path, \"reports.txt\")\n",
        "    if not os.path.exists(reports_file):\n",
        "        logger.error(f\"Reports file not found at {reports_file}\")\n",
        "        raise FileNotFoundError(f\"Reports file not found at {reports_file}\")\n",
        "\n",
        "    try:\n",
        "        logger.info(\"Loading radiology reports dataset...\")\n",
        "        dataset = load_dataset(\"text\", data_files={\"train\": reports_file}, split=\"train\")\n",
        "\n",
        "        logger.info(\"Loading GPT-2 model and tokenizer...\")\n",
        "        tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "        model = AutoModelForCausalLM.from_pretrained(\"gpt2\").to(device)\n",
        "\n",
        "        def preprocess_function(examples):\n",
        "            return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=512)\n",
        "\n",
        "        logger.info(\"Preprocessing dataset...\")\n",
        "        tokenized_dataset = dataset.map(preprocess_function, batched=True)\n",
        "\n",
        "        logger.info(\"Setting up training arguments...\")\n",
        "        training_args = TrainingArguments(\n",
        "            output_dir=\"./gpt2_results\",\n",
        "            evaluation_strategy=\"epoch\",\n",
        "            learning_rate=5e-5,\n",
        "            per_device_train_batch_size=4,\n",
        "            num_train_epochs=1,  # Set to 1 for demonstration; increase for real training\n",
        "            save_steps=10_000,\n",
        "            save_total_limit=2,\n",
        "        )\n",
        "\n",
        "        logger.info(\"Starting fine-tuning...\")\n",
        "        trainer = Trainer(\n",
        "            model=model,\n",
        "            args=training_args,\n",
        "            train_dataset=tokenized_dataset,\n",
        "        )\n",
        "        trainer.train()\n",
        "\n",
        "        logger.info(\"Fine-tuning complete. Saving model...\")\n",
        "        model.save_pretrained(GPT2_MODEL_PATH)\n",
        "        tokenizer.save_pretrained(GPT2_MODEL_PATH)\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Failed to fine-tune GPT-2 model: {e}\")\n",
        "        raise\n",
        "\n",
        "# Step 3: Create a Medical Image Analyzer class that can use either fine-tuned or pretrained models\n",
        "class MedicalImageAnalyzer:\n",
        "    def __init__(self, use_fine_tuned=True):\n",
        "        \"\"\"\n",
        "        Initialize the analyzer with either fine-tuned or pretrained models.\n",
        "\n",
        "        Args:\n",
        "            use_fine_tuned: If True, try to load fine-tuned models. If they don't exist,\n",
        "                            fall back to pretrained models.\n",
        "        \"\"\"\n",
        "        self.use_fine_tuned = use_fine_tuned\n",
        "\n",
        "        # Load BLIP model\n",
        "        logger.info(\"Loading BLIP model...\")\n",
        "        if use_fine_tuned and os.path.exists(BLIP_MODEL_PATH):\n",
        "            try:\n",
        "                self.image_processor = BlipProcessor.from_pretrained(BLIP_MODEL_PATH)\n",
        "                self.image_model = BlipForConditionalGeneration.from_pretrained(BLIP_MODEL_PATH).to(device)\n",
        "                logger.info(\"Successfully loaded fine-tuned BLIP model\")\n",
        "            except Exception as e:\n",
        "                logger.warning(f\"Error loading fine-tuned BLIP model: {e}. Falling back to pretrained model.\")\n",
        "                self.image_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "                self.image_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\").to(device)\n",
        "        else:\n",
        "            logger.info(\"Using pretrained BLIP model\")\n",
        "            self.image_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "            self.image_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\").to(device)\n",
        "\n",
        "        # Load GPT-2 model\n",
        "        logger.info(\"Loading GPT-2 model...\")\n",
        "        if use_fine_tuned and os.path.exists(GPT2_MODEL_PATH):\n",
        "            try:\n",
        "                self.nlp_tokenizer = AutoTokenizer.from_pretrained(GPT2_MODEL_PATH)\n",
        "                self.nlp_model = AutoModelForCausalLM.from_pretrained(GPT2_MODEL_PATH).to(device)\n",
        "                logger.info(\"Successfully loaded fine-tuned GPT-2 model\")\n",
        "            except Exception as e:\n",
        "                logger.warning(f\"Error loading fine-tuned GPT-2 model: {e}. Falling back to pretrained model.\")\n",
        "                self.nlp_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "                self.nlp_model = AutoModelForCausalLM.from_pretrained(\"gpt2\").to(device)\n",
        "        else:\n",
        "            logger.info(\"Using pretrained GPT-2 model\")\n",
        "            self.nlp_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "            self.nlp_model = AutoModelForCausalLM.from_pretrained(\"gpt2\").to(device)\n",
        "\n",
        "    def analyze_image(self, image: Image.Image, clinical_context: str = \"\") -> dict:\n",
        "        \"\"\"\n",
        "        Analyze a medical image with optional clinical context.\n",
        "\n",
        "        Args:\n",
        "            image: PIL Image object\n",
        "            clinical_context: Optional string with clinical information\n",
        "\n",
        "        Returns:\n",
        "            dict: Analysis report with findings and recommendations\n",
        "        \"\"\"\n",
        "        try:\n",
        "            if image is None:\n",
        "                return {\"error\": \"No image provided\"}\n",
        "\n",
        "            # Preprocess image\n",
        "            processed_image = self._preprocess_image(image)\n",
        "\n",
        "            # Generate findings\n",
        "            findings = self._generate_findings(processed_image, clinical_context)\n",
        "\n",
        "            # Generate recommendations\n",
        "            recommendations = self._generate_recommendations(findings)\n",
        "\n",
        "            # Structure the final report\n",
        "            report = self._structure_report(clinical_context, findings, recommendations)\n",
        "\n",
        "            return report\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Analysis failed: {str(e)}\")\n",
        "            return {\"error\": str(e)}\n",
        "\n",
        "    def _preprocess_image(self, image: Image.Image) -> Image.Image:\n",
        "        \"\"\"Preprocess the image for model input.\"\"\"\n",
        "        # Handle different image modes\n",
        "        if image.mode != \"RGB\":\n",
        "            image = image.convert(\"RGB\")\n",
        "        # Fix orientation based on EXIF data\n",
        "        return ImageOps.exif_transpose(image)\n",
        "\n",
        "    def _generate_findings(self, image: Image.Image, context: str) -> str:\n",
        "        \"\"\"Generate findings from the image using the BLIP model.\"\"\"\n",
        "        prompt = (\n",
        "            f\"Analyze the medical image and provide a detailed report. \"\n",
        "            f\"Clinical context: {context if context else 'None provided'}. \"\n",
        "            \"Include details about anatomical structures, abnormalities, and technical quality.\"\n",
        "        )\n",
        "        # Process inputs\n",
        "        inputs = self.image_processor(image, text=prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "        # Generate output\n",
        "        with torch.no_grad():\n",
        "            outputs = self.image_model.generate(**inputs, max_new_tokens=200)\n",
        "\n",
        "        # Decode output\n",
        "        findings = self.image_processor.decode(outputs[0], skip_special_tokens=True)\n",
        "        return findings.strip()\n",
        "\n",
        "    def _generate_recommendations(self, findings: str) -> str:\n",
        "        \"\"\"Generate recommendations based on findings using the GPT-2 model.\"\"\"\n",
        "        prompt = (\n",
        "            f\"Based on the following findings:\\n{findings}\\n\\n\"\n",
        "            \"Provide 3-5 clinical recommendations. Categorize them as urgent or routine.\"\n",
        "        )\n",
        "        # Process inputs\n",
        "        inputs = self.nlp_tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "        # Generate output\n",
        "        with torch.no_grad():\n",
        "            outputs = self.nlp_model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=150,\n",
        "                temperature=0.7,\n",
        "                do_sample=True,\n",
        "                top_p=0.95\n",
        "            )\n",
        "\n",
        "        # Decode output\n",
        "        recommendations = self.nlp_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        # Extract only the generated recommendations (remove the prompt)\n",
        "        recommendations = recommendations[len(prompt):].strip()\n",
        "        return recommendations\n",
        "\n",
        "    def _structure_report(self, context: str, findings: str, recommendations: str) -> dict:\n",
        "        \"\"\"Structure the analysis results into a report.\"\"\"\n",
        "        return {\n",
        "            \"metadata\": {\n",
        "                \"report_date\": datetime.now().isoformat(),\n",
        "                \"analysis_version\": \"1.0\",\n",
        "                \"model_type\": \"fine-tuned\" if self.use_fine_tuned else \"pretrained\",\n",
        "            },\n",
        "            \"clinical_context\": context,\n",
        "            \"findings\": findings,\n",
        "            \"recommendations\": recommendations,\n",
        "        }\n",
        "\n",
        "# Step 4: Create functions for Gradio interface\n",
        "def initialize_analyzer():\n",
        "    \"\"\"Initialize the analyzer once and return it for reuse.\"\"\"\n",
        "    # Check if fine-tuned models exist, otherwise use pretrained models\n",
        "    use_fine_tuned = os.path.exists(BLIP_MODEL_PATH) and os.path.exists(GPT2_MODEL_PATH)\n",
        "    return MedicalImageAnalyzer(use_fine_tuned=use_fine_tuned)\n",
        "\n",
        "# Global variable to store the analyzer once initialized\n",
        "analyzer = None\n",
        "\n",
        "def analyze_medical_image(image, clinical_context=\"\"):\n",
        "    \"\"\"Function to analyze medical image for Gradio interface.\"\"\"\n",
        "    global analyzer\n",
        "\n",
        "    try:\n",
        "        # Initialize analyzer if not already done\n",
        "        if analyzer is None:\n",
        "            analyzer = initialize_analyzer()\n",
        "\n",
        "        if image is None:\n",
        "            return \"❌ Error: No image provided. Please upload an image first.\"\n",
        "\n",
        "        # Analyze image\n",
        "        report = analyzer.analyze_image(image, clinical_context)\n",
        "\n",
        "        if \"error\" in report:\n",
        "            return f\"❌ Error: {report['error']}\"\n",
        "\n",
        "        # Format the report for display\n",
        "        return (\n",
        "            f\"**Clinical Context**: {clinical_context if clinical_context else 'None provided'}\\n\\n\"\n",
        "            f\"**Findings**:\\n{report['findings']}\\n\\n\"\n",
        "            f\"**Recommendations**:\\n{report['recommendations']}\\n\\n\"\n",
        "            f\"**Note**: This analysis was generated using \"\n",
        "            f\"{'fine-tuned' if analyzer.use_fine_tuned else 'pretrained'} models.\\n\"\n",
        "            f\"This analysis requires verification by a qualified radiologist.\"\n",
        "        )\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error during analysis: {str(e)}\")\n",
        "        return f\"❌ Error during analysis: {str(e)}\"\n",
        "\n",
        "def get_model_status():\n",
        "    \"\"\"Get the status of the models (fine-tuned or pretrained).\"\"\"\n",
        "    blip_status = \"Fine-tuned\" if os.path.exists(BLIP_MODEL_PATH) else \"Pretrained\"\n",
        "    gpt2_status = \"Fine-tuned\" if os.path.exists(GPT2_MODEL_PATH) else \"Pretrained\"\n",
        "    return f\"BLIP Model: {blip_status}\\nGPT-2 Model: {gpt2_status}\"\n",
        "\n",
        "def start_fine_tuning():\n",
        "    \"\"\"Start the fine-tuning process.\"\"\"\n",
        "    try:\n",
        "        fine_tune_blip()\n",
        "        fine_tune_gpt2()\n",
        "        # Reinitialize analyzer with newly fine-tuned models\n",
        "        global analyzer\n",
        "        analyzer = initialize_analyzer()\n",
        "        return get_model_status() + \"\\n\\n✅ Fine-tuning completed successfully! Models are ready to use.\"\n",
        "    except Exception as e:\n",
        "        return f\"❌ Fine-tuning failed: {str(e)}\"\n",
        "\n",
        "# Step 5: Create Gradio interface\n",
        "def create_gradio_app():\n",
        "    \"\"\"Create and launch the Gradio application.\"\"\"\n",
        "    with gr.Blocks() as app:\n",
        "        gr.Markdown(\"# 🩺 Medical Image Analysis\")\n",
        "\n",
        "        with gr.Row():\n",
        "            with gr.Column():\n",
        "                # Model status display\n",
        "                status_output = gr.Markdown(value=get_model_status())\n",
        "                fine_tune_btn = gr.Button(\"Fine-tune Models\", variant=\"secondary\")\n",
        "\n",
        "                # User inputs\n",
        "                image_input = gr.Image(type=\"pil\", label=\"Upload Medical Image\")\n",
        "                context_input = gr.Textbox(\n",
        "                    label=\"Clinical Context (optional)\",\n",
        "                    placeholder=\"Patient symptoms or history...\",\n",
        "                    lines=3\n",
        "                )\n",
        "                analyze_btn = gr.Button(\"Analyze Image\", variant=\"primary\")\n",
        "\n",
        "            with gr.Column():\n",
        "                # Analysis output\n",
        "                report_output = gr.Markdown(label=\"Analysis Report\")\n",
        "\n",
        "        # Set up event handlers\n",
        "        fine_tune_btn.click(\n",
        "            start_fine_tuning,\n",
        "            inputs=[],\n",
        "            outputs=[status_output]\n",
        "        )\n",
        "\n",
        "        analyze_btn.click(\n",
        "            analyze_medical_image,\n",
        "            inputs=[image_input, context_input],\n",
        "            outputs=[report_output]\n",
        "        )\n",
        "\n",
        "        # In newer Gradio versions, we don't use update() method\n",
        "        # Instead, we set the initial value when creating the component\n",
        "\n",
        "    return app\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Create and launch app\n",
        "    app = create_gradio_app()\n",
        "    app.launch(server_name=\"0.0.0.0\", server_port=200, share=True)"
      ],
      "metadata": {
        "id": "eAwfaZAOiTFI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        " # Install required libraries\n",
        "!pip install gradio transformers torch pillow datasets kaggle\n",
        "\n",
        "# Install Kaggle for dataset download (if needed)\n",
        "try:\n",
        "    import kaggle\n",
        "    print(\"Kaggle already installed\")\n",
        "except ImportError:\n",
        "    print(\"Installing Kaggle...\")\n",
        "    !pip install kaggle\n",
        "\n",
        "# Import necessary libraries\n",
        "import os\n",
        "import logging\n",
        "import re\n",
        "from datetime import datetime\n",
        "from PIL import Image, ImageOps\n",
        "import torch\n",
        "from transformers import (\n",
        "    BlipProcessor,\n",
        "    BlipForConditionalGeneration,\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        ")\n",
        "from datasets import load_dataset\n",
        "import gradio as gr\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n",
        ")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Set device\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "logger.info(f\"Using device: {device}\")\n",
        "\n",
        "# Define paths\n",
        "BLIP_MODEL_PATH = \"./fine_tuned_blip\"\n",
        "GPT2_MODEL_PATH = \"./fine_tuned_gpt2\"\n",
        "DATASET_PATH = \"/content/spinal_cord_dataset\"  # Local path where dataset will be downloaded/stored\n",
        "\n",
        "# Function to download the dataset from Kaggle if not already present\n",
        "def download_dataset(force_download=False):\n",
        "    \"\"\"\n",
        "    Download the spinal cord dataset from Kaggle if not already downloaded.\n",
        "\n",
        "    Args:\n",
        "        force_download: If True, redownload even if files exist\n",
        "\n",
        "    Returns:\n",
        "        str: Path to the downloaded dataset\n",
        "    \"\"\"\n",
        "    if os.path.exists(DATASET_PATH) and not force_download:\n",
        "        logger.info(f\"Dataset already exists at {DATASET_PATH}\")\n",
        "        return DATASET_PATH\n",
        "\n",
        "    try:\n",
        "        # Check if Kaggle credentials exist\n",
        "        import kaggle\n",
        "        logger.info(\"Attempting to download dataset from Kaggle...\")\n",
        "\n",
        "        # Create dataset directory if it doesn't exist\n",
        "        os.makedirs(DATASET_PATH, exist_ok=True)\n",
        "\n",
        "        # Download the dataset using the Kaggle API\n",
        "        # Note: This requires a kaggle.json file in ~/.kaggle/ with API credentials\n",
        "        try:\n",
        "            # Try to download directly using the kaggle API\n",
        "            kaggle.api.authenticate()\n",
        "            kaggle.api.dataset_download_files(\n",
        "                \"trainingdatapro/spinal-cord-dataset\",\n",
        "                path=DATASET_PATH,\n",
        "                unzip=True\n",
        "            )\n",
        "            logger.info(f\"Successfully downloaded dataset to {DATASET_PATH}\")\n",
        "            return DATASET_PATH\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Failed to download using API: {e}\")\n",
        "\n",
        "            # Alternative method using shell commands\n",
        "            logger.info(\"Trying alternative download method...\")\n",
        "            try:\n",
        "                !mkdir -p {DATASET_PATH}\n",
        "                !kaggle datasets download -d trainingdatapro/spinal-cord-dataset -p {DATASET_PATH}\n",
        "                !unzip {DATASET_PATH}/spinal-cord-dataset.zip -d {DATASET_PATH}\n",
        "                logger.info(f\"Successfully downloaded dataset to {DATASET_PATH}\")\n",
        "                return DATASET_PATH\n",
        "            except Exception as e2:\n",
        "                logger.error(f\"Failed to download dataset: {e2}\")\n",
        "                raise\n",
        "    except ImportError:\n",
        "        logger.error(\"Kaggle not installed or configured properly\")\n",
        "        logger.info(\"Please install Kaggle and set up credentials manually:\")\n",
        "        logger.info(\"1. pip install kaggle\")\n",
        "        logger.info(\"2. Create ~/.kaggle/kaggle.json with your API key from kaggle.com/account\")\n",
        "        logger.info(\"3. chmod 600 ~/.kaggle/kaggle.json\")\n",
        "        logger.info(f\"4. Download dataset from kaggle.com/trainingdatapro/spinal-cord-dataset and extract to {DATASET_PATH}\")\n",
        "        raise\n",
        "\n",
        "    return DATASET_PATH\n",
        "\n",
        "# Step 1: Fine-Tune the BLIP Model for Image Analysis\n",
        "def fine_tune_blip(dataset_path=DATASET_PATH, force_retrain=False):\n",
        "    \"\"\"Fine-tune BLIP model if not already fine-tuned or if force_retrain is True.\"\"\"\n",
        "    if os.path.exists(BLIP_MODEL_PATH) and not force_retrain:\n",
        "        logger.info(f\"BLIP model already fine-tuned at {BLIP_MODEL_PATH}. Skipping training.\")\n",
        "        return\n",
        "\n",
        "    # Download/check dataset\n",
        "    try:\n",
        "        dataset_path = download_dataset()\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Failed to download dataset: {e}\")\n",
        "        raise\n",
        "\n",
        "    # Ensure dataset directory exists\n",
        "    if not os.path.exists(dataset_path):\n",
        "        logger.error(f\"Dataset not found at {dataset_path}. Please download the dataset first.\")\n",
        "        raise FileNotFoundError(f\"Dataset not found at {dataset_path}\")\n",
        "\n",
        "    try:\n",
        "        logger.info(\"Loading spinal cord dataset...\")\n",
        "\n",
        "        # Try to determine the dataset structure\n",
        "        # First, check if there's an images folder\n",
        "        images_folder = os.path.join(dataset_path, \"images\")\n",
        "        if os.path.exists(images_folder):\n",
        "            logger.info(f\"Found images folder at {images_folder}\")\n",
        "            dataset_path = images_folder\n",
        "\n",
        "        # Check if there are image files directly in the dataset folder\n",
        "        image_files = [f for f in os.listdir(dataset_path) if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tiff'))]\n",
        "        if image_files:\n",
        "            logger.info(f\"Found {len(image_files)} image files in {dataset_path}\")\n",
        "        else:\n",
        "            logger.warning(f\"No image files found in {dataset_path}\")\n",
        "            # Look for subdirectories that might contain images\n",
        "            for root, dirs, files in os.walk(dataset_path):\n",
        "                images = [f for f in files if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tiff'))]\n",
        "                if images:\n",
        "                    logger.info(f\"Found {len(images)} images in {root}\")\n",
        "                    dataset_path = root\n",
        "                    break\n",
        "\n",
        "        # Try to load as image folder dataset\n",
        "        try:\n",
        "            dataset = load_dataset(\"imagefolder\", data_dir=dataset_path, split=\"train[:1%]\")\n",
        "            logger.info(f\"Successfully loaded dataset as imagefolder from {dataset_path}\")\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"Failed to load dataset as imagefolder: {e}\")\n",
        "            # Try to load from a text file with image paths\n",
        "            try:\n",
        "                # Look for a metadata file or labels file\n",
        "                metadata_files = [f for f in os.listdir(dataset_path) if f.lower().endswith(('.csv', '.txt'))]\n",
        "                if metadata_files:\n",
        "                    logger.info(f\"Found metadata files: {metadata_files}\")\n",
        "                    # For now, just use the images directly, without labels\n",
        "\n",
        "                # Fallback to creating a simple dataset from image files\n",
        "                image_files = [os.path.join(dataset_path, f) for f in os.listdir(dataset_path)\n",
        "                              if f.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tiff'))]\n",
        "\n",
        "                if not image_files:\n",
        "                    raise ValueError(f\"No image files found in {dataset_path}\")\n",
        "\n",
        "                logger.info(f\"Creating dataset from {len(image_files)} image files\")\n",
        "\n",
        "                # Create a simple dataset with image paths\n",
        "                from datasets import Dataset\n",
        "                dataset = Dataset.from_dict({\n",
        "                    \"image\": image_files,\n",
        "                    \"label\": [\"\"] * len(image_files)  # Empty labels for now\n",
        "                })\n",
        "\n",
        "                # Load images\n",
        "                def load_image(example):\n",
        "                    example[\"image\"] = Image.open(example[\"image\"]).convert(\"RGB\")\n",
        "                    return example\n",
        "\n",
        "                dataset = dataset.map(load_image)\n",
        "                logger.info(\"Successfully created dataset from image files\")\n",
        "            except Exception as e2:\n",
        "                logger.error(f\"Failed to create dataset: {e2}\")\n",
        "                raise\n",
        "\n",
        "        logger.info(\"Loading BLIP model and processor...\")\n",
        "        processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "        model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\").to(device)\n",
        "\n",
        "        def preprocess_function(examples):\n",
        "            # Ensure 'label' exists in the dataset, otherwise use empty strings\n",
        "            labels = examples.get(\"label\", [\"\"] * len(examples[\"image\"]))\n",
        "            # Process images and text\n",
        "            try:\n",
        "                inputs = processor(images=examples[\"image\"], text=labels, return_tensors=\"pt\", padding=True)\n",
        "                return inputs\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Error processing examples: {e}\")\n",
        "                # Print example structure for debugging\n",
        "                logger.info(f\"Example keys: {examples.keys()}\")\n",
        "                logger.info(f\"Image type: {type(examples['image'][0])}\")\n",
        "                raise\n",
        "\n",
        "        logger.info(\"Preprocessing dataset...\")\n",
        "        # Get dataset column names before mapping\n",
        "        column_names = dataset.column_names\n",
        "        logger.info(f\"Dataset column names: {column_names}\")\n",
        "\n",
        "        tokenized_dataset = dataset.map(preprocess_function, batched=True, remove_columns=column_names)\n",
        "\n",
        "        logger.info(\"Setting up training arguments...\")\n",
        "        training_args = TrainingArguments(\n",
        "            output_dir=\"./blip_results\",\n",
        "            evaluation_strategy=\"epoch\",\n",
        "            learning_rate=5e-5,\n",
        "            per_device_train_batch_size=4,\n",
        "            num_train_epochs=1,  # Set to 1 for demonstration; increase for real training\n",
        "            save_steps=10_000,\n",
        "            save_total_limit=2,\n",
        "            remove_unused_columns=False,\n",
        "        )\n",
        "\n",
        "        logger.info(\"Starting fine-tuning...\")\n",
        "        trainer = Trainer(\n",
        "            model=model,\n",
        "            args=training_args,\n",
        "            train_dataset=tokenized_dataset,\n",
        "        )\n",
        "        trainer.train()\n",
        "\n",
        "        logger.info(\"Fine-tuning complete. Saving model...\")\n",
        "        model.save_pretrained(BLIP_MODEL_PATH)\n",
        "        processor.save_pretrained(BLIP_MODEL_PATH)\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Failed to fine-tune BLIP model: {e}\")\n",
        "        raise\n",
        "\n",
        "# Step 2: Fine-Tune the GPT-2 Model for Text Generation\n",
        "def fine_tune_gpt2(dataset_path=DATASET_PATH, force_retrain=False):\n",
        "    \"\"\"Fine-tune GPT-2 model if not already fine-tuned or if force_retrain is True.\"\"\"\n",
        "    if os.path.exists(GPT2_MODEL_PATH) and not force_retrain:\n",
        "        logger.info(f\"GPT-2 model already fine-tuned at {GPT2_MODEL_PATH}. Skipping training.\")\n",
        "        return\n",
        "\n",
        "    # Download/check dataset\n",
        "    try:\n",
        "        dataset_path = download_dataset()\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Failed to download dataset: {e}\")\n",
        "        raise\n",
        "\n",
        "    # Ensure dataset directory exists\n",
        "    if not os.path.exists(dataset_path):\n",
        "        logger.error(f\"Dataset not found at {dataset_path}. Please download the dataset first.\")\n",
        "        raise FileNotFoundError(f\"Dataset not found at {dataset_path}\")\n",
        "\n",
        "    try:\n",
        "        logger.info(\"Looking for text data for radiology reports...\")\n",
        "\n",
        "        # Try to find text files containing reports\n",
        "        reports_file = None\n",
        "        reports_content = []\n",
        "\n",
        "        # Search for potential report files\n",
        "        for root, dirs, files in os.walk(dataset_path):\n",
        "            for filename in files:\n",
        "                # Look for text files that might contain reports\n",
        "                if filename.lower().endswith('.txt') and 'report' in filename.lower():\n",
        "                    reports_file = os.path.join(root, filename)\n",
        "                    logger.info(f\"Found potential reports file: {reports_file}\")\n",
        "                    break\n",
        "                # Also consider CSV files that might contain report data\n",
        "                elif filename.lower().endswith('.csv') and 'report' in filename.lower():\n",
        "                    reports_file = os.path.join(root, filename)\n",
        "                    logger.info(f\"Found potential reports CSV: {reports_file}\")\n",
        "                    break\n",
        "\n",
        "            if reports_file:\n",
        "                break\n",
        "\n",
        "        # If no specific reports file is found, try to extract text from any text files\n",
        "        if not reports_file:\n",
        "            logger.info(\"No specific reports file found, searching for any text files...\")\n",
        "            text_files = []\n",
        "            for root, dirs, files in os.walk(dataset_path):\n",
        "                for filename in files:\n",
        "                    if filename.lower().endswith('.txt'):\n",
        "                        text_files.append(os.path.join(root, filename))\n",
        "\n",
        "            if text_files:\n",
        "                logger.info(f\"Found {len(text_files)} general text files\")\n",
        "                reports_file = text_files[0]  # Use the first text file\n",
        "            else:\n",
        "                logger.warning(\"No text files found in the dataset\")\n",
        "\n",
        "        # If we found a reports file, read its content\n",
        "        if reports_file:\n",
        "            logger.info(f\"Reading reports from {reports_file}\")\n",
        "            try:\n",
        "                with open(reports_file, 'r', encoding='utf-8') as f:\n",
        "                    content = f.read()\n",
        "                    reports_content = [content]\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Error reading reports file: {e}\")\n",
        "                raise\n",
        "\n",
        "        # If no reports file or content, create synthetic data for demonstration\n",
        "        if not reports_content:\n",
        "            logger.warning(\"No reports content found. Creating synthetic data for demonstration purposes.\")\n",
        "            reports_content = [\n",
        "                \"FINDINGS: MRI of the thoracic spine demonstrates normal alignment. The vertebral body heights are preserved. \"\n",
        "                \"There is no evidence of compression fracture or significant degenerative changes. \"\n",
        "                \"The spinal cord demonstrates normal signal intensity throughout the thoracic region. \"\n",
        "                \"No evidence of cord compression, edema, or syrinx formation. \"\n",
        "                \"The intervertebral discs demonstrate normal signal intensity and height. \"\n",
        "                \"No evidence of significant disc bulge or herniation. \"\n",
        "                \"The paravertebral soft tissues are normal in appearance.\",\n",
        "\n",
        "                \"FINDINGS: MRI of the lumbar spine reveals mild degenerative disc disease at L4-L5 and L5-S1 levels. \"\n",
        "                \"There is a small central disc protrusion at L4-L5 without significant neural compression. \"\n",
        "                \"No evidence of spinal stenosis. The vertebral body heights are maintained. \"\n",
        "                \"The conus medullaris terminates at an appropriate level and appears normal in signal intensity. \"\n",
        "                \"The paraspinal soft tissues are unremarkable.\",\n",
        "\n",
        "                \"FINDINGS: Cervical spine MRI demonstrates straightening of the normal cervical lordosis, \"\n",
        "                \"suggestive of muscle spasm. Mild disc desiccation is noted at C5-C6 and C6-C7 levels. \"\n",
        "                \"There is a small right paracentral disc protrusion at C5-C6 causing mild right foraminal narrowing. \"\n",
        "                \"No significant central canal stenosis. The spinal cord demonstrates normal signal intensity. \"\n",
        "                \"No evidence of cord compression or myelomalacia.\"\n",
        "            ]\n",
        "\n",
        "        # Create a dataset from the reports\n",
        "        from datasets import Dataset\n",
        "        dataset = Dataset.from_dict({\"text\": reports_content})\n",
        "\n",
        "        logger.info(\"Loading GPT-2 model and tokenizer...\")\n",
        "        tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "        tokenizer.pad_token = tokenizer.eos_token  # Set pad token for GPT-2\n",
        "        model = AutoModelForCausalLM.from_pretrained(\"gpt2\").to(device)\n",
        "\n",
        "        def preprocess_function(examples):\n",
        "            return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=512)\n",
        "\n",
        "        logger.info(\"Preprocessing dataset...\")\n",
        "        tokenized_dataset = dataset.map(preprocess_function, batched=True)\n",
        "\n",
        "        logger.info(\"Setting up training arguments...\")\n",
        "        training_args = TrainingArguments(\n",
        "            output_dir=\"./gpt2_results\",\n",
        "            evaluation_strategy=\"epoch\",\n",
        "            learning_rate=5e-5,\n",
        "            per_device_train_batch_size=4,\n",
        "            num_train_epochs=1,  # Set to 1 for demonstration; increase for real training\n",
        "            save_steps=10_000,\n",
        "            save_total_limit=2,\n",
        "        )\n",
        "\n",
        "        logger.info(\"Starting fine-tuning...\")\n",
        "        trainer = Trainer(\n",
        "            model=model,\n",
        "            args=training_args,\n",
        "            train_dataset=tokenized_dataset,\n",
        "        )\n",
        "        trainer.train()\n",
        "\n",
        "        logger.info(\"Fine-tuning complete. Saving model...\")\n",
        "        model.save_pretrained(GPT2_MODEL_PATH)\n",
        "        tokenizer.save_pretrained(GPT2_MODEL_PATH)\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Failed to fine-tune GPT-2 model: {e}\")\n",
        "        raise\n",
        "\n",
        "# Step 3: Create a Medical Image Analyzer class that can use either fine-tuned or pretrained models\n",
        "class MedicalImageAnalyzer:\n",
        "    def __init__(self, use_fine_tuned=True):\n",
        "        \"\"\"\n",
        "        Initialize the analyzer with either fine-tuned or pretrained models.\n",
        "\n",
        "        Args:\n",
        "            use_fine_tuned: If True, try to load fine-tuned models. If they don't exist,\n",
        "                            fall back to pretrained models.\n",
        "        \"\"\"\n",
        "        self.use_fine_tuned = use_fine_tuned\n",
        "\n",
        "        # Load BLIP model\n",
        "        logger.info(\"Loading BLIP model...\")\n",
        "        if use_fine_tuned and os.path.exists(BLIP_MODEL_PATH):\n",
        "            try:\n",
        "                self.image_processor = BlipProcessor.from_pretrained(BLIP_MODEL_PATH)\n",
        "                self.image_model = BlipForConditionalGeneration.from_pretrained(BLIP_MODEL_PATH).to(device)\n",
        "                logger.info(\"Successfully loaded fine-tuned BLIP model\")\n",
        "            except Exception as e:\n",
        "                logger.warning(f\"Error loading fine-tuned BLIP model: {e}. Falling back to pretrained model.\")\n",
        "                self.image_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "                self.image_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\").to(device)\n",
        "        else:\n",
        "            logger.info(\"Using pretrained BLIP model\")\n",
        "            self.image_processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "            self.image_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\").to(device)\n",
        "\n",
        "        # Load GPT-2 model\n",
        "        logger.info(\"Loading GPT-2 model...\")\n",
        "        if use_fine_tuned and os.path.exists(GPT2_MODEL_PATH):\n",
        "            try:\n",
        "                self.nlp_tokenizer = AutoTokenizer.from_pretrained(GPT2_MODEL_PATH)\n",
        "                self.nlp_model = AutoModelForCausalLM.from_pretrained(GPT2_MODEL_PATH).to(device)\n",
        "                logger.info(\"Successfully loaded fine-tuned GPT-2 model\")\n",
        "            except Exception as e:\n",
        "                logger.warning(f\"Error loading fine-tuned GPT-2 model: {e}. Falling back to pretrained model.\")\n",
        "                self.nlp_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "                self.nlp_model = AutoModelForCausalLM.from_pretrained(\"gpt2\").to(device)\n",
        "        else:\n",
        "            logger.info(\"Using pretrained GPT-2 model\")\n",
        "            self.nlp_tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "            self.nlp_model = AutoModelForCausalLM.from_pretrained(\"gpt2\").to(device)\n",
        "\n",
        "    def analyze_image(self, image: Image.Image, clinical_context: str = \"\") -> dict:\n",
        "        \"\"\"\n",
        "        Analyze a medical image with optional clinical context.\n",
        "\n",
        "        Args:\n",
        "            image: PIL Image object\n",
        "            clinical_context: Optional string with clinical information\n",
        "\n",
        "        Returns:\n",
        "            dict: Analysis report with findings and recommendations\n",
        "        \"\"\"\n",
        "        try:\n",
        "            if image is None:\n",
        "                return {\"error\": \"No image provided\"}\n",
        "\n",
        "            # Preprocess image\n",
        "            processed_image = self._preprocess_image(image)\n",
        "\n",
        "            # Generate findings\n",
        "            findings = self._generate_findings(processed_image, clinical_context)\n",
        "\n",
        "            # Generate recommendations\n",
        "            recommendations = self._generate_recommendations(findings)\n",
        "\n",
        "            # Structure the final report\n",
        "            report = self._structure_report(clinical_context, findings, recommendations)\n",
        "\n",
        "            return report\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Analysis failed: {str(e)}\")\n",
        "            return {\"error\": str(e)}\n",
        "\n",
        "    def _preprocess_image(self, image: Image.Image) -> Image.Image:\n",
        "        \"\"\"Preprocess the image for model input.\"\"\"\n",
        "        # Handle different image modes\n",
        "        if image.mode != \"RGB\":\n",
        "            image = image.convert(\"RGB\")\n",
        "        # Fix orientation based on EXIF data\n",
        "        return ImageOps.exif_transpose(image)\n",
        "\n",
        "    def _generate_findings(self, image: Image.Image, context: str) -> str:\n",
        "        \"\"\"Generate findings from the image using the BLIP model.\"\"\"\n",
        "        prompt = (\n",
        "            f\"Analyze the medical image and provide a detailed report. \"\n",
        "            f\"Clinical context: {context if context else 'None provided'}. \"\n",
        "            \"Include details about anatomical structures, abnormalities, and technical quality.\"\n",
        "        )\n",
        "        # Process inputs\n",
        "        inputs = self.image_processor(image, text=prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "        # Generate output\n",
        "        with torch.no_grad():\n",
        "            outputs = self.image_model.generate(**inputs, max_new_tokens=200)\n",
        "\n",
        "        # Decode output\n",
        "        findings = self.image_processor.decode(outputs[0], skip_special_tokens=True)\n",
        "        return findings.strip()\n",
        "\n",
        "    def _generate_recommendations(self, findings: str) -> str:\n",
        "        \"\"\"Generate recommendations based on findings using the GPT-2 model.\"\"\"\n",
        "        prompt = (\n",
        "            f\"Based on the following findings:\\n{findings}\\n\\n\"\n",
        "            \"Provide 3-5 clinical recommendations. Categorize them as urgent or routine.\"\n",
        "        )\n",
        "        # Process inputs\n",
        "        inputs = self.nlp_tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "        # Generate output\n",
        "        with torch.no_grad():\n",
        "            outputs = self.nlp_model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=150,\n",
        "                temperature=0.7,\n",
        "                do_sample=True,\n",
        "                top_p=0.95\n",
        "            )\n",
        "\n",
        "        # Decode output\n",
        "        recommendations = self.nlp_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        # Extract only the generated recommendations (remove the prompt)\n",
        "        recommendations = recommendations[len(prompt):].strip()\n",
        "        return recommendations\n",
        "\n",
        "    def _structure_report(self, context: str, findings: str, recommendations: str) -> dict:\n",
        "        \"\"\"Structure the analysis results into a report.\"\"\"\n",
        "        return {\n",
        "            \"metadata\": {\n",
        "                \"report_date\": datetime.now().isoformat(),\n",
        "                \"analysis_version\": \"1.0\",\n",
        "                \"model_type\": \"fine-tuned\" if self.use_fine_tuned else \"pretrained\",\n",
        "            },\n",
        "            \"clinical_context\": context,\n",
        "            \"findings\": findings,\n",
        "            \"recommendations\": recommendations,\n",
        "        }\n",
        "\n",
        "# Step 4: Create functions for Gradio interface\n",
        "def initialize_analyzer():\n",
        "    \"\"\"Initialize the analyzer once and return it for reuse.\"\"\"\n",
        "    # Check if fine-tuned models exist, otherwise use pretrained models\n",
        "    use_fine_tuned = os.path.exists(BLIP_MODEL_PATH) and os.path.exists(GPT2_MODEL_PATH)\n",
        "    return MedicalImageAnalyzer(use_fine_tuned=use_fine_tuned)\n",
        "\n",
        "# Global variable to store the analyzer once initialized\n",
        "analyzer = None\n",
        "\n",
        "def analyze_medical_image(image, clinical_context=\"\"):\n",
        "    \"\"\"Function to analyze medical image for Gradio interface.\"\"\"\n",
        "    global analyzer\n",
        "\n",
        "    try:\n",
        "        # Initialize analyzer if not already done\n",
        "        if analyzer is None:\n",
        "            analyzer = initialize_analyzer()\n",
        "\n",
        "        if image is None:\n",
        "            return \"❌ Error: No image provided. Please upload an image first.\"\n",
        "\n",
        "        # Analyze image\n",
        "        report = analyzer.analyze_image(image, clinical_context)\n",
        "\n",
        "        if \"error\" in report:\n",
        "            return f\"❌ Error: {report['error']}\"\n",
        "\n",
        "        # Format the report for display\n",
        "        return (\n",
        "            f\"**Clinical Context**: {clinical_context if clinical_context else 'None provided'}\\n\\n\"\n",
        "            f\"**Findings**:\\n{report['findings']}\\n\\n\"\n",
        "            f\"**Recommendations**:\\n{report['recommendations']}\\n\\n\"\n",
        "            f\"**Note**: This analysis was generated using \"\n",
        "            f\"{'fine-tuned' if analyzer.use_fine_tuned else 'pretrained'} models.\\n\"\n",
        "            f\"This analysis requires verification by a qualified radiologist.\"\n",
        "        )\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error during analysis: {str(e)}\")\n",
        "        return f\"❌ Error during analysis: {str(e)}\"\n",
        "\n",
        "def get_model_status():\n",
        "    \"\"\"Get the status of the models (fine-tuned or pretrained).\"\"\"\n",
        "    blip_status = \"Fine-tuned\" if os.path.exists(BLIP_MODEL_PATH) else \"Pretrained\"\n",
        "    gpt2_status = \"Fine-tuned\" if os.path.exists(GPT2_MODEL_PATH) else \"Pretrained\"\n",
        "    return f\"BLIP Model: {blip_status}\\nGPT-2 Model: {gpt2_status}\"\n",
        "\n",
        "def start_fine_tuning():\n",
        "    \"\"\"Start the fine-tuning process.\"\"\"\n",
        "    try:\n",
        "        fine_tune_blip()\n",
        "        fine_tune_gpt2()\n",
        "        # Reinitialize analyzer with newly fine-tuned models\n",
        "        global analyzer\n",
        "        analyzer = initialize_analyzer()\n",
        "        return get_model_status() + \"\\n\\n✅ Fine-tuning completed successfully! Models are ready to use.\"\n",
        "    except Exception as e:\n",
        "        return f\"❌ Fine-tuning failed: {str(e)}\"\n",
        "\n",
        "# Step 5: Create Gradio interface\n",
        "def create_gradio_app():\n",
        "    \"\"\"Create and launch the Gradio application.\"\"\"\n",
        "    with gr.Blocks() as app:\n",
        "        gr.Markdown(\"# 🩺 Medical Image Analysis\")\n",
        "\n",
        "        with gr.Row():\n",
        "            with gr.Column():\n",
        "                # Model status display\n",
        "                status_output = gr.Markdown(value=get_model_status())\n",
        "                fine_tune_btn = gr.Button(\"Fine-tune Models\", variant=\"secondary\")\n",
        "\n",
        "                # User inputs\n",
        "                image_input = gr.Image(type=\"pil\", label=\"Upload Medical Image\")\n",
        "                context_input = gr.Textbox(\n",
        "                    label=\"Clinical Context (optional)\",\n",
        "                    placeholder=\"Patient symptoms or history...\",\n",
        "                    lines=3\n",
        "                )\n",
        "                analyze_btn = gr.Button(\"Analyze Image\", variant=\"primary\")\n",
        "\n",
        "            with gr.Column():\n",
        "                # Analysis output\n",
        "                report_output = gr.Markdown(label=\"Analysis Report\")\n",
        "\n",
        "        # Set up event handlers\n",
        "        fine_tune_btn.click(\n",
        "            start_fine_tuning,\n",
        "            inputs=[],\n",
        "            outputs=[status_output]\n",
        "        )\n",
        "\n",
        "        analyze_btn.click(\n",
        "            analyze_medical_image,\n",
        "            inputs=[image_input, context_input],\n",
        "            outputs=[report_output]\n",
        "        )\n",
        "\n",
        "        # In newer Gradio versions, we don't use update() method\n",
        "        # Instead, we set the initial value when creating the component\n",
        "\n",
        "    return app\n",
        "\n",
        "# Main execution\n",
        "if __name__ == \"__main__\":\n",
        "    # Create and launch app\n",
        "    app = create_gradio_app()\n",
        "    app.launch(server_name=\"0.0.0.0\", server_port=12, share=True)"
      ],
      "metadata": {
        "id": "29CI022W65Qb",
        "outputId": "1a1cdef0-247d-4aa9-896d-cc2758810685",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gradio in /usr/local/lib/python3.11/dist-packages (5.29.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (11.2.1)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.14.4)\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.11/dist-packages (1.7.4.5)\n",
            "Requirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (24.1.0)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.115.12)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.11/dist-packages (from gradio) (0.5.0)\n",
            "Requirement already satisfied: gradio-client==1.10.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.10.0)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.31.1)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.18)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.11.4)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.11.9)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.46.2)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.13.2)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.15.3)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.13.2)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.34.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.0->gradio) (2025.3.2)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.0->gradio) (15.0.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.11/dist-packages (from kaggle) (6.2.0)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2025.4.26)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.4.2)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from kaggle) (3.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from kaggle) (5.29.4)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.9.0.post0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.11/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.11/dist-packages (from kaggle) (75.2.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.17.0)\n",
            "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.11/dist-packages (from kaggle) (1.3)\n",
            "Requirement already satisfied: urllib3>=1.15.1 in /usr/local/lib/python3.11/dist-packages (from kaggle) (2.4.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.11/dist-packages (from kaggle) (0.5.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.16.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (1.1.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Kaggle already installed\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://d9435ec922b7b8ccbe.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://d9435ec922b7b8ccbe.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from kaggle.api.kaggle_api_extended import KaggleApi\n",
        "\n",
        "# Set up Kaggle API credentials\n",
        "os.environ[\"KAGGLE_USERNAME\"] = \"sujitnirmal\"\n",
        "os.environ[\"KAGGLE_KEY\"] = \"591a53c307288d0b46fc0a7e25d60531\"\n",
        "\n",
        "# Initialize the Kaggle API\n",
        "api = KaggleApi()\n",
        "api.authenticate()\n",
        "\n",
        "# Download the spinal-cord-dataset\n",
        "dataset_name = \"charan3341/mimic-cxrt\"\n",
        "download_path = \"./spinal_cord_dataset\"\n",
        "api.dataset_download_files(dataset_name, path=download_path, unzip=True)\n",
        "\n",
        "print(f\"Dataset downloaded and extracted to: {download_path}\")"
      ],
      "metadata": {
        "id": "5DNjMc8L_0fu",
        "outputId": "199c2c59-3f7e-4ac3-e6a6-ea216acb20c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 388
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/charan3341/mimic-cxrt\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "HTTPError",
          "evalue": "403 Client Error: Forbidden for url: https://www.kaggle.com/api/v1/datasets/download/charan3341/mimic-cxrt?raw=false",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-f344f5363cd8>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mdataset_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"charan3341/mimic-cxrt\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mdownload_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"./spinal_cord_dataset\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mapi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_download_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdownload_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munzip\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Dataset downloaded and extracted to: {download_path}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/kaggle/api/kaggle_api_extended.py\u001b[0m in \u001b[0;36mdataset_download_files\u001b[0;34m(self, dataset, path, force, quiet, unzip, licenses)\u001b[0m\n\u001b[1;32m   1662\u001b[0m       \u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_slug\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset_slug\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1663\u001b[0m       \u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_version_number\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset_version_number\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1664\u001b[0;31m       \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkaggle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_api_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1666\u001b[0m       \u001b[0moutfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meffective_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_slug\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'.zip'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/kagglesdk/datasets/services/dataset_api_service.py\u001b[0m in \u001b[0;36mdownload_dataset\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m     78\u001b[0m       \u001b[0mrequest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mApiDownloadDatasetRequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"datasets.DatasetApiService\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ApiDownloadDataset\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHttpRedirect\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mdownload_dataset_raw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mApiDownloadDatasetRawRequest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mHttpRedirect\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/kagglesdk/kaggle_http_client.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, service_name, request_name, request, response_type)\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0mhttp_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_request\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhttp_response\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/kagglesdk/kaggle_http_client.py\u001b[0m in \u001b[0;36m_prepare_response\u001b[0;34m(self, response_type, http_response)\u001b[0m\n\u001b[1;32m    189\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_prepare_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhttp_response\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_print_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_response\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m     \u001b[0mhttp_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'application/json'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mhttp_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Content-Type'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m       \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhttp_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1026\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHTTPError\u001b[0m: 403 Client Error: Forbidden for url: https://www.kaggle.com/api/v1/datasets/download/charan3341/mimic-cxrt?raw=false"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# mimic_cxr_path = kagglehub.dataset_download('charan3341/mimic-cxr')"
      ],
      "metadata": {
        "id": "mkSLFOLfESBz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}